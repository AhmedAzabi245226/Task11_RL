from __future__ import annotations

import os
import sys
import argparse
import shutil
import gc
from datetime import datetime
from pathlib import Path
from typing import Any, Optional

# -------------------------------------------------
# Force correct project root
# -------------------------------------------------
THIS_DIR = Path(__file__).resolve().parent
PROJECT_ROOT = THIS_DIR.parent
os.chdir(str(PROJECT_ROOT))
sys.path.insert(0, str(PROJECT_ROOT))

from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import BaseCallback
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.vec_env import DummyVecEnv, VecMonitor
from envs.ot2_gym_wrapper import OT2GymEnv

# ----------------------------
# Args
# ----------------------------
def parse_args():
    p = argparse.ArgumentParser()

    # ClearML
    p.add_argument("--use_clearml", action="store_true", help="Enqueue training remotely via ClearML")
    p.add_argument("--project_name", type=str, default="Mentor Group - Alican/Group 1")
    p.add_argument("--task_name", type=str, default="OT2_PPO_Train")
    p.add_argument("--queue", type=str, default="default")
    p.add_argument("--docker", type=str, default="deanis/2023y2b-rl:latest")

    # Run bookkeeping
    p.add_argument("--run_name", type=str, default="", help="Folder name under models/. If empty, autogenerated.")
    p.add_argument("--seed", type=int, default=0)

    # Env
    p.add_argument("--max_steps", type=int, default=400)
    p.add_argument("--success_threshold", type=float, default=0.03)
    p.add_argument("--action_repeat", type=int, default=5)
    p.add_argument("--render", action="store_true", help="Enable rendering (not recommended on server)")

    # PPO hyperparameters
    p.add_argument("--learning_rate", type=float, default=3e-4)
    p.add_argument("--batch_size", type=int, default=256)
    p.add_argument("--n_steps", type=int, default=2048)
    p.add_argument("--n_epochs", type=int, default=10)
    p.add_argument("--gamma", type=float, default=0.99)
    p.add_argument("--clip_range", type=float, default=0.2)
    p.add_argument("--ent_coef", type=float, default=0.0)

    # Timesteps
    p.add_argument("--total_timesteps", type=int, default=204_800)
    p.add_argument("--checkpoint_freq", type=int, default=102_400)

    # Resume / staged training
    p.add_argument("--resume_path", type=str, default="", help="Path to PPO .zip on WORKER or repo-relative (NOT a URL).")
    p.add_argument("--resume_task_id", type=str, default="", help="ClearML task id to pull resume model from.")
    p.add_argument(
        "--resume_artifact",
        type=str,
        default="ppo_final_model",
        help="Artifact name in resume task (default: ppo_final_model).",
    )
    p.add_argument(
        "--reset_timesteps",
        action="store_true",
        help="Reset SB3 timesteps counter at first learn() call (default: continue).",
    )
    p.add_argument(
        "--recreate_env_each_chunk",
        action="store_true",
        help="Close/recreate env between chunks (helps avoid long-run PyBullet memory creep).",
    )

    # Upload controls (IMPORTANT)
    p.add_argument("--upload_checkpoints", action="store_true", help="Upload checkpoint artifacts to ClearML (OFF by default).")
    p.add_argument("--upload_final", action="store_true", help="Upload final model artifact to ClearML (ON by default).")
    p.add_argument("--upload_run_zip", action="store_true", help="Upload run folder zip to ClearML (OFF by default).")
    p.set_defaults(upload_checkpoints=False, upload_final=True, upload_run_zip=False)

    # If upload_checkpoints is enabled, reduce frequency if needed
    p.add_argument("--upload_every_n_checkpoints", type=int, default=1,
                   help="If uploading checkpoints, only upload every Nth checkpoint (default: 1 = all).")

    return p.parse_args()


def round_up_to_multiple(x: int, m: int) -> int:
    x = int(x)
    m = int(m)
    return int(((x + m - 1) // m) * m)


def _cast_like(current_value: Any, new_value: Any) -> Any:
    """Cast new_value to the type of current_value where possible (for ClearML cfg sync)."""
    try:
        if isinstance(current_value, bool):
            if isinstance(new_value, str):
                return new_value.strip().lower() in ("1", "true", "yes", "y", "on")
            return bool(new_value)
        if isinstance(current_value, int):
            return int(new_value)
        if isinstance(current_value, float):
            return float(new_value)
        return new_value
    except Exception:
        return new_value


# ----------------------------
# ClearML helpers (robust)
# ----------------------------
def _is_worker() -> bool:
    return bool(os.environ.get("CLEARML_TASK_ID") or os.environ.get("CLEARML_WORKER_ID"))


def _get_task():
    try:
        from clearml import Task
        t = Task.current_task()
        if t is None:
            tid = os.environ.get("CLEARML_TASK_ID")
            if tid:
                t = Task.get_task(task_id=tid)
        return t
    except Exception:
        return None


def clearml_setup_and_sync_args(args) -> Optional[str]:
    """
    Prevent recursion:
    - Worker: attach to current task and sync args from ClearML.
    - Local + --use_clearml: create task, connect args, enqueue, then EXIT immediately.
    """
    if not args.use_clearml and not _is_worker():
        return None

    from clearml import Task

    if _is_worker():
        args.use_clearml = True
        task = _get_task()
        if task is None:
            tid = os.environ.get("CLEARML_TASK_ID")
            if not tid:
                raise RuntimeError("Worker detected but CLEARML_TASK_ID missing; cannot attach task.")
            task = Task.get_task(task_id=tid)

        cfg = task.connect(vars(args), name="cli_args")
        for k, v in cfg.items():
            if hasattr(args, k):
                setattr(args, k, _cast_like(getattr(args, k), v))

        print(f"[ClearML] Worker attached to task_id={task.id}")
        return task.id

    task = Task.init(project_name=args.project_name, task_name=args.task_name)
    task.connect(vars(args), name="cli_args")
    task.set_base_docker(args.docker)

    print("[ClearML] Enqueuing task to queue:", args.queue)
    task.execute_remotely(queue_name=args.queue)
    raise SystemExit(0)


# ----------------------------
# Memory cleanup
# ----------------------------
def _cleanup_memory(tag: str = ""):
    try:
        gc.collect()
    except Exception:
        pass
    try:
        import torch
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
    except Exception:
        pass
    if tag:
        print(f"[mem] cleanup done: {tag}")


# ----------------------------
# Upload (safe)
# ----------------------------
def upload_artifact(name: str, filepath: str):
    """
    Upload artifact. Best-effort, never crash training if upload fails.
    """
    try:
        task = _get_task()
        if task is None:
            print(f"[upload_artifact] No current ClearML task. Skipping {name}.")
            return
        if not filepath or not os.path.exists(filepath):
            print(f"[upload_artifact] Missing file for {name}: {filepath}")
            return

        size_mb = os.path.getsize(filepath) / (1024 * 1024)
        print(f"[upload_artifact] Uploading {name}: {filepath} ({size_mb:.2f} MB)")

        # Blocking upload tends to be more stable on long jobs
        task.upload_artifact(name=name, artifact_object=filepath, wait_on_upload=True)
        try:
            task.flush(wait_for_uploads=True)
        except Exception:
            pass

        _cleanup_memory(tag=f"after upload {name}")

    except Exception as e:
        print(f"[upload_artifact] FAILED {name}: {e}")


# ----------------------------
# Resume resolution
# ----------------------------
def resolve_resume_local_path(args) -> str:
    if args.resume_path and (args.resume_path.startswith("http://") or args.resume_path.startswith("https://")):
        raise RuntimeError(
            "resume_path is an HTTP(S) URL. Use ClearML artifact resume instead:\n"
            "--resume_task_id <TASK_ID> --resume_artifact ppo_final_model"
        )

    if args.resume_path:
        print("[resume] CWD:", os.getcwd())
        print("[resume] PROJECT_ROOT:", str(PROJECT_ROOT))
        print("[resume] resume_path (raw):", args.resume_path)

        if os.path.exists(args.resume_path):
            return args.resume_path

        candidate = os.path.join(str(PROJECT_ROOT), args.resume_path)
        print("[resume] trying PROJECT_ROOT-resolved:", candidate)
        if os.path.exists(candidate):
            return candidate

        folder = os.path.join(str(PROJECT_ROOT), os.path.dirname(args.resume_path))
        if os.path.isdir(folder):
            try:
                print("[resume] listing:", folder, "->", os.listdir(folder))
            except Exception:
                pass

        raise FileNotFoundError(f"--resume_path not found. Checked: {args.resume_path} and {candidate}")

    if args.resume_task_id:
        from clearml import Task
        t = Task.get_task(task_id=args.resume_task_id)
        if args.resume_artifact not in t.artifacts:
            raise RuntimeError(
                f"Artifact '{args.resume_artifact}' not found in task {args.resume_task_id}. "
                f"Available: {list(t.artifacts.keys())}"
            )
        local = t.artifacts[args.resume_artifact].get_local_copy()
        print("[resume] Pulled from ClearML artifact:", local)
        return local

    return ""


# ----------------------------
# Scalars
# ----------------------------
class ClearMLScalarCallback(BaseCallback):
    def __init__(self, report_every_steps: int = 2048):
        super().__init__()
        self.report_every_steps = int(report_every_steps)
        self._logger = None
        self._ep_return = 0.0
        self._ep_returns = []
        self._successes = 0
        self._episodes = 0

    def _on_training_start(self) -> None:
        try:
            task = _get_task()
            self._logger = task.get_logger() if task else None
            print("[ClearML] logger active:", bool(self._logger))
        except Exception:
            self._logger = None
            print("[ClearML] logger active: False (exception)")

    def _on_step(self) -> bool:
        reward = float(self.locals["rewards"][0])
        self._ep_return += reward

        dones = self.locals["dones"]
        infos = self.locals["infos"]

        if bool(dones[0]):
            self._episodes += 1
            self._ep_returns.append(self._ep_return)
            if bool(infos[0].get("is_success", False)):
                self._successes += 1
            self._ep_return = 0.0

        if self._logger and (self.num_timesteps % self.report_every_steps == 0):
            window = 50
            recent = self._ep_returns[-window:] if self._ep_returns else []
            ep_rew_mean = sum(recent) / len(recent) if recent else 0.0
            success_rate = (self._successes / self._episodes) if self._episodes > 0 else 0.0
            it = int(self.num_timesteps)

            self._logger.report_scalar("rollout", "ep_rew_mean_est", ep_rew_mean, iteration=it)
            self._logger.report_scalar("rollout", "success_rate_est", success_rate, iteration=it)
            self._logger.report_scalar("time", "timesteps", it, iteration=it)

        return True


# ----------------------------
# VecEnv factory (important!)
# ----------------------------
def make_vec_env(args) -> VecMonitor:
    def _make() -> OT2GymEnv:
        env = OT2GymEnv(
            render=args.render,
            max_steps=args.max_steps,
            success_threshold=args.success_threshold,
            seed=args.seed,
            debug=False,
            action_repeat=args.action_repeat,
        )
        # Monitor records episode stats properly and tends to close cleanly
        return Monitor(env)

    vec = DummyVecEnv([_make])
    vec = VecMonitor(vec)
    return vec


def safe_close_vec_env(vec_env):
    try:
        if vec_env is not None:
            vec_env.close()
    except Exception:
        pass
    _cleanup_memory(tag="after vec_env.close()")


# ----------------------------
# Main
# ----------------------------
def main():
    args = parse_args()
    print("ARGV:", sys.argv)

    current_task_id = clearml_setup_and_sync_args(args)

    if not args.run_name:
        stamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        args.run_name = f"ppo_ot2_{stamp}"

    rollout = int(args.n_steps)
    args.total_timesteps = round_up_to_multiple(args.total_timesteps, rollout)
    args.checkpoint_freq = round_up_to_multiple(args.checkpoint_freq, rollout)

    model_root = os.path.join(str(PROJECT_ROOT), "models", args.run_name)
    os.makedirs(model_root, exist_ok=True)

    total = int(args.total_timesteps)
    chunk = int(args.checkpoint_freq) if int(args.checkpoint_freq) > 0 else total

    resume_local = resolve_resume_local_path(args)

    print("\n========== TRAINING CONFIG ==========")
    print("Running remotely (worker detected):", _is_worker())
    print("ClearML task id:", current_task_id if current_task_id else "(none)")
    print("Project:", args.project_name)
    print("Task name:", args.task_name)
    print("Queue:", args.queue)
    print("Run name:", args.run_name)
    print("Total timesteps (rounded):", total)
    print("Checkpoint freq (rounded):", chunk)
    print("PPO n_steps:", args.n_steps)
    print("Batch size:", args.batch_size)
    print("Env max_steps:", args.max_steps)
    print("Env success_threshold:", args.success_threshold)
    print("Env action_repeat:", args.action_repeat)
    print("Resume local:", resume_local if resume_local else "(none)")
    print("Recreate env each chunk:", bool(args.recreate_env_each_chunk))
    print("UPLOAD checkpoints:", bool(args.upload_checkpoints))
    print("UPLOAD final:", bool(args.upload_final))
    print("UPLOAD run zip:", bool(args.upload_run_zip))
    print("Save dir:", model_root)
    print("====================================\n")

    vec_env = make_vec_env(args)

    if resume_local:
        print("Resuming PPO from:", resume_local)
        model = PPO.load(resume_local, env=vec_env, device="auto")
    else:
        model = PPO(
            "MlpPolicy",
            vec_env,
            verbose=1,
            learning_rate=args.learning_rate,
            batch_size=args.batch_size,
            n_steps=args.n_steps,
            n_epochs=args.n_epochs,
            gamma=args.gamma,
            clip_range=args.clip_range,
            ent_coef=args.ent_coef,
        )

    cb = ClearMLScalarCallback(report_every_steps=args.n_steps)

    trained = 0
    use_progress_bar = not _is_worker()
    ckpt_index = 0

    while trained < total:
        this_chunk = min(chunk, total - trained)

        model.learn(
            total_timesteps=this_chunk,
            progress_bar=use_progress_bar,
            reset_num_timesteps=bool(args.reset_timesteps),
            callback=cb,
        )
        args.reset_timesteps = False
        trained += this_chunk
        ckpt_index += 1

        ckpt_base = os.path.join(model_root, f"ppo_ot2_{trained}_steps")
        ckpt_zip = f"{ckpt_base}.zip"
        print("Saving checkpoint to:", ckpt_base)
        model.save(ckpt_base)
        print("Checkpoint exists?", os.path.exists(ckpt_zip), ckpt_zip)

        # ---- SAFE UPLOAD STRATEGY ----
        # Uploading while Bullet is alive is where you keep crashing.
        # If checkpoint uploads are enabled, we close vec_env BEFORE upload, then recreate it after.
        do_upload = (
            args.upload_checkpoints
            and (ckpt_index % max(1, int(args.upload_every_n_checkpoints)) == 0)
        )
        if do_upload:
            print("[checkpoint-upload] Closing env before upload for stability...")
            safe_close_vec_env(vec_env)

            upload_artifact(name=f"ppo_checkpoint_{trained}_steps", filepath=ckpt_zip)

            print("[checkpoint-upload] Recreating env after upload...")
            vec_env = make_vec_env(args)
            model.set_env(vec_env)

        # Optional env recreation even if not uploading (helps Bullet memory creep)
        if args.recreate_env_each_chunk and not do_upload:
            print("[env] Recreating env between chunks...")
            safe_close_vec_env(vec_env)
            vec_env = make_vec_env(args)
            model.set_env(vec_env)

        _cleanup_memory(tag=f"after chunk {trained}")

    # Final save
    final_base = os.path.join(model_root, "ppo_ot2_final")
    final_zip = f"{final_base}.zip"
    print("Saving final model to:", final_base)
    model.save(final_base)
    print("Final model exists?", os.path.exists(final_zip), final_zip)

    # Close env BEFORE final uploads (most stable)
    safe_close_vec_env(vec_env)

    if args.upload_final:
        upload_artifact(name="ppo_final_model", filepath=final_zip)

    if args.upload_run_zip:
        try:
            zip_base = os.path.join(str(PROJECT_ROOT), "models", args.run_name)
            zip_file = shutil.make_archive(base_name=zip_base, format="zip", root_dir=model_root)
            print("Run folder zip exists?", os.path.exists(zip_file), zip_file)
            upload_artifact(name="run_folder_zip", filepath=zip_file)
        except Exception as e:
            print("[zip] FAILED:", e)

    _cleanup_memory(tag="final")

    print("\nTraining finished successfully.")
    print("All checkpoints saved under:", model_root)
    print("Download from ClearML: Task -> Artifacts")


if __name__ == "__main__":
    main()
